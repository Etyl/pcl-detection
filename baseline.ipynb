{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports + GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from copy import deepcopy\n",
    "from urllib import request\n",
    "from dont_patronize_me import DontPatronizeMe # data manager module\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "from preprocessing import load_data, preprocess_data, DPMDataset\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA A100 80GB PCIe MIG 1g.10gb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df, test_df = load_data()\n",
    "\n",
    "# downsample negative instances\n",
    "pcldf = train_df[train_df.label==1]\n",
    "npos = len(pcldf)\n",
    "balanced_train_df = pd.concat([pcldf, train_df[train_df.label==0][:int(2.5*npos)]])\n",
    "balanced_train_df = balanced_train_df[['text', 'community', 'label', 'country']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_df = preprocess_data(balanced_train_df, clean_data=False, augment_data=False, add_country=False, add_community=False)\n",
    "processed_dev_df = preprocess_data(dev_df, clean_data=False, add_country=False, add_community=False)\n",
    "processed_test_df = preprocess_data(test_df, clean_data=False, add_country=False, add_community=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW with logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.3516\n",
      "Accuracy: 0.8080\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.84      0.89      1895\n",
      "           1       0.26      0.55      0.35       199\n",
      "\n",
      "    accuracy                           0.81      2094\n",
      "   macro avg       0.60      0.69      0.62      2094\n",
      "weighted avg       0.88      0.81      0.84      2094\n",
      "\n",
      "\n",
      "Example of misclassified text:\n",
      "                                                                                                                                                                                                                                                        text  \\\n",
      "1424  \"In an article posted to its English language website on Thursday , China 's state-run Xinhua news agency sang the praises of microblogs , saying that China 's some 195 million microbloggers have \"\" become strong force to help those in need . \"\"\"   \n",
      "\n",
      "      label  \n",
      "1424      0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "# Data\n",
    "X_train, y_train = processed_train_df[\"text\"], processed_train_df[\"label\"]\n",
    "X_dev, y_dev = processed_dev_df[\"text\"], processed_dev_df[\"label\"]\n",
    "\n",
    "# Transform text in BoW\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3), max_features=5000)  # Uni-gram + Bi-gram\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_dev_bow = vectorizer.transform(X_dev)\n",
    "\n",
    "# Logistic regression\n",
    "logreg = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "logreg.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = logreg.predict(X_dev_bow)\n",
    "\n",
    "# Evaluation\n",
    "f1 = f1_score(y_dev, y_pred)\n",
    "acc = accuracy_score(y_dev, y_pred)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_dev, y_pred))\n",
    "\n",
    "# Find misclassified examples\n",
    "misclassified = processed_dev_df.iloc[(y_pred != y_dev).to_numpy()]\n",
    "\n",
    "# Print one example\n",
    "example = misclassified.sample(1, random_state=42)\n",
    "print(\"\\nExample of misclassified text:\")\n",
    "print(example[[\"text\", \"label\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF with logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score with TF-IDF: 0.3755\n",
      "\n",
      "Example of misclassified text:\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               text  \\\n",
      "1249  The casting was gut-punchingly faithful and , with very few exceptions , matched up with the pictures in my head , the story stayed true , and somehow managed to unfold in a way that entirely satisfied the vast majority of those who loved the books without leaving those who had n't ( yet ) done so hopelessly lost . It was a Hollywood miracle -- and finally , I had the ultimate challenge for those still wary of the written word : Watch the series , and then tell me you do n't want to know what happens next . I dare you .   \n",
      "\n",
      "      label  \n",
      "1249      0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "# Data\n",
    "X_train, y_train = processed_train_df[\"text\"], processed_train_df[\"label\"]\n",
    "X_dev, y_dev = processed_dev_df[\"text\"], processed_dev_df[\"label\"]\n",
    "\n",
    "# Transform text in BoW\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=10000)  \n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_dev_tfidf = vectorizer.transform(X_dev)\n",
    "\n",
    "# Logistic regression\n",
    "logreg = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = logreg.predict(X_dev_tfidf)\n",
    "f1_tfidf = f1_score(y_dev, y_pred)\n",
    "print(f\"F1 Score with TF-IDF: {f1_tfidf:.4f}\")\n",
    "\n",
    "# Find misclassified examples\n",
    "misclassified = processed_dev_df.iloc[(y_pred != y_dev).to_numpy()]\n",
    "\n",
    "# Print one example\n",
    "example = misclassified.sample(1, random_state=42)\n",
    "print(\"\\nExample of misclassified text:\")\n",
    "print(example[[\"text\", \"label\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF with Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score with Na√Øve Bayes: 0.0853\n",
      "\n",
      "Example of misclassified text:\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               text  \\\n",
      "1249  The casting was gut-punchingly faithful and , with very few exceptions , matched up with the pictures in my head , the story stayed true , and somehow managed to unfold in a way that entirely satisfied the vast majority of those who loved the books without leaving those who had n't ( yet ) done so hopelessly lost . It was a Hollywood miracle -- and finally , I had the ultimate challenge for those still wary of the written word : Watch the series , and then tell me you do n't want to know what happens next . I dare you .   \n",
      "1301                                                                                                                                                                                                                                     This poignant ad boils everything down to the bones . An X-ray is the great leveller and the ad shows different types of love between people : heterosexual , homosexual , young , old , able , disabled , friends , family , and all castes and creeds . For diversity and inclusion , this ad nails it .   \n",
      "474                                               Affirmative Consent The aliens are clever . They have arrived at Earth at a point when the human race is at its most vulnerable , but they have n't picked a period of military weakness where the planet could be taken over in a violent invasion . Instead , they chose a time when humans are at their greatest need -- sudden , apocalyptic , and hopeless need . They will take over the world without firing a shot . In fact , they will take over the world because we 'll ask them to .   \n",
      "\n",
      "      label  \n",
      "1249      0  \n",
      "1301      0  \n",
      "474       0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Data\n",
    "X_train, y_train = processed_train_df[\"text\"], processed_train_df[\"label\"]\n",
    "X_dev, y_dev = processed_dev_df[\"text\"], processed_dev_df[\"label\"]\n",
    "\n",
    "# Transform text in BoW\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=10000)  \n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_dev_tfidf = vectorizer.transform(X_dev)\n",
    "\n",
    "# Na√Øve Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred_nb = nb_model.predict(X_dev_tfidf)\n",
    "f1_nb = f1_score(y_dev, y_pred_nb)\n",
    "print(f\"F1 Score with Na√Øve Bayes: {f1_nb:.4f}\")\n",
    "\n",
    "# Find misclassified examples\n",
    "misclassified = processed_dev_df.iloc[(y_pred != y_dev).to_numpy()]\n",
    "\n",
    "# Print one example\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "example = misclassified.sample(3, random_state=42)\n",
    "print(\"\\nExample of misclassified text:\")\n",
    "print(example[[\"text\", \"label\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score with SVM: 0.3871\n",
      "\n",
      "Example of misclassified text:\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               text  \\\n",
      "1249  The casting was gut-punchingly faithful and , with very few exceptions , matched up with the pictures in my head , the story stayed true , and somehow managed to unfold in a way that entirely satisfied the vast majority of those who loved the books without leaving those who had n't ( yet ) done so hopelessly lost . It was a Hollywood miracle -- and finally , I had the ultimate challenge for those still wary of the written word : Watch the series , and then tell me you do n't want to know what happens next . I dare you .   \n",
      "\n",
      "      label  \n",
      "1249      0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Data\n",
    "X_train, y_train = processed_train_df[\"text\"], processed_train_df[\"label\"]\n",
    "X_dev, y_dev = processed_dev_df[\"text\"], processed_dev_df[\"label\"]\n",
    "\n",
    "# Transform text in BoW\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=10000)  \n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_dev_tfidf = vectorizer.transform(X_dev)\n",
    "\n",
    "# SVM model\n",
    "svm_model = LinearSVC(class_weight=\"balanced\", random_state=42)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred_svm = svm_model.predict(X_dev_tfidf)\n",
    "f1_svm = f1_score(y_dev, y_pred_svm)\n",
    "print(f\"F1 Score with SVM: {f1_svm:.4f}\")\n",
    "\n",
    "# Find misclassified examples\n",
    "misclassified = processed_dev_df.iloc[(y_pred != y_dev).to_numpy()]\n",
    "\n",
    "# Print one example\n",
    "example = misclassified.sample(1, random_state=42)\n",
    "print(\"\\nExample of misclassified text:\")\n",
    "print(example[[\"text\", \"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Top 20 Bigrams in False Positives:**\n",
      "poor families: 6.58\n",
      "homeless people: 2.29\n",
      "children poor: 1.28\n",
      "young people: 1.25\n",
      "vulnerable children: 1.12\n",
      "homeless man: 1.12\n",
      "year old: 1.11\n",
      "come poor: 1.06\n",
      "homeless person: 1.03\n",
      "disabled children: 0.99\n",
      "women children: 0.96\n",
      "people need: 0.94\n",
      "refugee camps: 0.89\n",
      "men women: 0.87\n",
      "asylum seekers: 0.85\n",
      "years ago: 0.84\n",
      "hopeless situation: 0.82\n",
      "illegal immigrants: 0.79\n",
      "help need: 0.79\n",
      "disabled people: 0.78\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# Identify false positives (FPs) where the model predicted PCL but the true label is 0\n",
    "false_positives = processed_dev_df[(y_pred_svm == 1) & (y_dev == 0)][\"text\"]\n",
    "\n",
    "# Vectorizer setup for bigrams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words=\"english\", max_features=5000)\n",
    "X_fp_tfidf = vectorizer.fit_transform(false_positives)\n",
    "\n",
    "# Get bigram feature names\n",
    "bigram_features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert TF-IDF matrix to a frequency count\n",
    "word_counts = np.asarray(X_fp_tfidf.sum(axis=0)).flatten()\n",
    "\n",
    "# Create a dictionary of bigram -> count\n",
    "bigram_freq = dict(zip(bigram_features, word_counts))\n",
    "\n",
    "# Sort bigrams by frequency\n",
    "sorted_bigrams = sorted(bigram_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the top 20 bigrams\n",
    "print(\"\\n**Top 20 Bigrams in False Positives:**\")\n",
    "for bigram, count in sorted_bigrams[:20]:\n",
    "    print(f\"{bigram}: {count:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
